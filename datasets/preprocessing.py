import torch
import numpy as np
import random
import librosa


class VideoPrep_MSC_CJ(object):
    def __init__(self,
                 crop=(224, 224),
                 color=(0.4, 0.4, 0.4, 0.2),
                 min_area=0.08,
                 augment=True,
                 normalize=True,
                 totensor=True,
                 num_frames=8,
                 pad_missing=False,
                 ):
        from utils.videotransforms import video_transforms, volume_transforms, tensor_transforms
        self.crop = crop
        self.augment = augment
        self.num_frames = num_frames
        self.pad_missing = pad_missing
        if normalize:
            assert totensor

        if augment:
            transforms = [
                video_transforms.RandomResizedCrop(crop, scale=(min_area, 1.)),
                video_transforms.RandomHorizontalFlip(),
                video_transforms.ColorJitter(*color),
            ]
        else:
            transforms = [
                video_transforms.Resize(int(crop[0]/0.875)),
                video_transforms.CenterCrop(crop),
            ]

        if totensor:
            transforms += [volume_transforms.ClipToTensor()]
            if normalize:
                transforms += [tensor_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
        self.transform = video_transforms.Compose(transforms)

    def __call__(self, frames):
        frames = self.transform(frames)
        if self.pad_missing:
            while True:
                n_missing = self.num_frames - frames.shape[1]
                if n_missing > 0:
                    frames = torch.cat((frames, frames[:, :n_missing]), 1)
                else:
                    break
        return frames


class VideoPrep_Crop_CJ(object):
    def __init__(self,
                 resize=(256, 256),
                 crop=(224, 224),
                 color=(0.4, 0.4, 0.4, 0.2),
                 num_frames=8,
                 pad_missing=False,
                 augment=True,
                 normalize=True,
                 totensor=True,
                 ):
        self.resize = resize
        self.crop = crop
        self.augment = augment
        self.num_frames = num_frames
        self.pad_missing = pad_missing
        if normalize:
            assert totensor

        from utils.videotransforms import video_transforms, volume_transforms, tensor_transforms

        if augment:
            transforms = [
                video_transforms.Resize(resize),
                video_transforms.RandomCrop(crop),
                video_transforms.RandomHorizontalFlip(),
                video_transforms.ColorJitter(*color),
            ]
        else:
            transforms = [
                video_transforms.Resize(resize),
                video_transforms.CenterCrop(crop),
            ]
        if totensor:
            transforms += [volume_transforms.ClipToTensor()]
            if normalize:
                transforms += [tensor_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
        self.transform = video_transforms.Compose(transforms)

    def __call__(self, frames):
        if isinstance(frames[0], list):
            return torch.stack([self(f) for f in frames])
        frames = self.transform(frames)
        if self.pad_missing:
            while True:
                n_missing = self.num_frames - frames.shape[1]
                if n_missing > 0:
                    frames = torch.cat((frames, frames[:, :n_missing]), 1)
                else:
                    break
        return frames


class AudioPreprocessing(object):
    def __init__(self, num_frames, augment=False):
        import torchaudio
        self.augment = augment
        self.trimpad = torchaudio.transforms.PadTrim(num_frames, channels_first=True)

    def __call__(self, sig, sr):
        # Downmix to mono
        sig = sig.mean(0).astype(np.float32)

        # Augment by changing volume +/- 10%
        if self.augment:
            vol_coeff = (random.random() * 0.2) + 0.9
            sig *= vol_coeff

        sig = torch.tensor(sig).unsqueeze(0)    # 1 x N
        return self.trimpad(sig)


def normalize_spectrogram(tensor, mean, std):
    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)
    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)
    tensor.sub_(mean[None, None, :]).div_(std[None, None, :])
    return tensor


class AudioPrepLibrosa(object):
    def __init__(self, trim_pad=True, duration=None, missing_as_zero=False, augment=False, to_tensor=False, volume=0.1):
        self.trim_pad = trim_pad
        self.missing_as_zero = missing_as_zero
        self.augment = augment
        self.to_tensor = to_tensor
        self.volume = volume
        if trim_pad:
            assert duration is not None
            self.duration = duration

    def __call__(self, sig, sr, duration=None):
        if duration is None:
            duration = self.duration
        num_frames = int(duration*sr)

        # Check if audio is missing
        if self.missing_as_zero and sig is None:
            sig = np.zeros((1, num_frames), dtype=np.float32)

        # Downmix to mono
        sig = sig.mean(0).astype(np.float32)

        # Trim or pad to constant shape
        if self.trim_pad:
            if sig.shape[0] > num_frames:
                sig = sig[:num_frames]
            elif sig.shape[0] < num_frames:
                n_pad = num_frames - sig.shape[0]
                sig = np.pad(sig, (0, n_pad), mode='constant', constant_values=(0., 0.))

        # Augment by changing volume +/- 10%
        if self.augment:
            sig *= random.uniform(1.-self.volume, 1.+self.volume)

        sig = sig[np.newaxis]
        if self.to_tensor:
            sig = torch.from_numpy(sig)

        return sig, sr


class LogSpectrogram(object):
    def __init__(self, fps, n_fft=512, hop_size=0.005, spect_db=True, normalize=False):
        self.inp_fps = fps
        self.n_fft = n_fft
        self.hop_size = hop_size
        self.rate = 1./hop_size
        self.spect_db = spect_db
        self.normalize = normalize

        if self.normalize:
            if self.spect_db:
                if n_fft == 512 and fps == 24000:
                    stats = np.load('datasets/assets/audio-spectDB-24k-513-norm-stats.npz')
                elif n_fft == 256 and fps == 24000:
                    stats = np.load('datasets/assets/audio-spectDB-24k-257-norm-stats.npz')
            else:
                stats = np.load('datasets/assets/audio-spect-librosa-norm-stats.npz')
            self.mean, self.std = stats['mean'], stats['std']

    def __call__(self, sig, sr, duration=None):
        hop_length = int(self.hop_size * sr)
        spect = np.abs(librosa.stft(sig[0], n_fft=self.n_fft*2, hop_length=hop_length))**2.
        spect = np.concatenate([spect[:1], spect[1:].reshape(self.n_fft//2, 2, -1).mean(1)], 0)
        if duration is not None:
            num_frames = int(duration * self.rate)
            spect = spect[:, :num_frames]

        if self.spect_db:
            spect = librosa.core.power_to_db(spect, top_db=100)
        if self.normalize:
            spect = (spect - self.mean[:, np.newaxis]) / (self.std[:, np.newaxis] + 1e-5)
        spect_tensor = torch.from_numpy(spect)
        spect_tensor = torch.transpose(spect_tensor, 0, 1).unsqueeze(0)
        return spect_tensor, self.rate

